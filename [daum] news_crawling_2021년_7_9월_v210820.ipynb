{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: © 2021 Hyunkyeong Ko <hyunkyeng925@naver.com>\n",
    " \n",
    "# SPDX-License-Identifier: BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 뉴스 스크래핑\n",
    "* 2021년 7월 ~ 2021년 9월"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver-autoinstaller in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromedriver-autoinstaller\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0] \n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe')   \n",
    "except:\n",
    "    chromedriver_autoinstaller.install(True)\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe')\n",
    "\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에 코드 에러 나면 이 코드 실행해주세요.\n",
    "\n",
    "#import shutil\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "#import subprocess\n",
    "\n",
    "#try:\n",
    "#    shutil.rmtree(r\"c:\\chrometemp\") \n",
    "#except FileNotFoundError:\n",
    "#    pass\n",
    "\n",
    "#subprocess.Popen(r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe --remote-debugging-port=9222 --user-data-dir=\"C:\\chrometemp\"') \n",
    "\n",
    "#option = Options()\n",
    "#option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "#chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]\n",
    "#try:\n",
    "#    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "#except:\n",
    "#    chromedriver_autoinstaller.install(True)\n",
    "#    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "#driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_url_list():\n",
    "    for i in range(20210701, 20210732):\n",
    "        etl_ymd = str(i)\n",
    "        url.append('https://search.daum.net/search?nil_suggest=btn&w=news&DA=PGD&q=%EC%BD%94%EB%A1%9C%EB%82%98&p=1&period=u&sd='+etl_ymd+'000000&ed='+etl_ymd+'235959')\n",
    "    return url\n",
    "\n",
    "url = []\n",
    "get_url_list()\n",
    "\n",
    "def get_article_url():\n",
    "    \n",
    "    urls_list = []\n",
    "    \n",
    "    for i in tqdm(range(1, 11)):\n",
    "        try:\n",
    "            order_num = str(i)\n",
    "            article_url = driver.find_elements_by_xpath('//*[@id=\"newsColl\"]/div[1]/ul/li['+order_num+']/div[2]/span[1]/a')\n",
    "            urls_list.append(article_url[0].get_attribute('href'))    \n",
    "            time.sleep(3)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return urls_list\n",
    "\n",
    "heads_list = []\n",
    "\n",
    "def get_article_head():\n",
    "    \n",
    "    try:\n",
    "        article_head = driver.find_elements_by_css_selector('#cSub > div > h3')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_head = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_head\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "def get_article_date():\n",
    "    \n",
    "    try:\n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        article_date = dom.find(\"span\", \"num_date\").text \n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_date = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_date\n",
    "\n",
    "press_list = []\n",
    "\n",
    "def get_press_name():\n",
    "\n",
    "    try:\n",
    "        press_name = driver.find_elements_by_css_selector('#cSub > div > em > a > img')[0].get_attribute(\"alt\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        press_name = ' '\n",
    "        pass\n",
    "    \n",
    "    return press_name\n",
    "\n",
    "contents_list = []\n",
    "\n",
    "def get_article_contents():\n",
    "    \n",
    "    try:\n",
    "        article_content = driver.find_elements_by_xpath('//*[@id=\"harmonyContainer\"]')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_content = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_content\n",
    "\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "def get_article_centiments():\n",
    "        \n",
    "    try:\n",
    "        article_centiment_like = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-LIKE.unselected > span.count')[0].text\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        article_centiment_like = ' '\n",
    "        pass\n",
    "    \n",
    "    time.sleep(3)\n",
    "        \n",
    "    try:\n",
    "        article_centiment_angry = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-ANGRY.unselected > span.count')[0].text\n",
    "        time.sleep(3)        \n",
    "    except:\n",
    "        article_centiment_angry = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_centiment_like, article_centiment_angry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▎                                                                          | 1/10 [00:10<01:30, 10.04s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:20<01:20, 10.04s/it]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:30<01:10, 10.05s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:40<01:00, 10.05s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:50<00:50, 10.04s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [01:00<00:40, 10.04s/it]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [01:10<00:30, 10.04s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:20<00:20, 10.04s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:30<00:10, 10.04s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:40<00:00, 10.04s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 1/5 [01:41<06:45, 101.39s/it]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▎                                                                          | 1/10 [00:00<00:01,  8.15it/s]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:10<00:11,  1.60s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:20<00:24,  4.12s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:30<00:29,  5.91s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:40<00:28,  7.14s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:50<00:13,  6.51s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:00<00:07,  7.57s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:10<00:00,  7.05s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [02:52<04:36, 92.27s/it]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▎                                                                          | 1/10 [00:10<01:30, 10.10s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:20<01:20, 10.09s/it]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:30<01:10, 10.08s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:40<01:00, 10.07s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:50<00:34,  8.56s/it]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [01:00<00:27,  9.01s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:10<00:18,  9.32s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:20<00:09,  9.54s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:30<00:00,  9.05s/it]\u001b[A\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [04:23<03:03, 91.91s/it]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:10<00:40,  5.03s/it]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:20<00:45,  6.53s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:30<00:45,  7.59s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:40<00:41,  8.38s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:50<00:35,  8.88s/it]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [01:00<00:27,  9.23s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:10<00:18,  9.47s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:20<00:09,  9.63s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:30<00:00,  9.05s/it]\u001b[A\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [05:55<01:31, 91.94s/it]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:00<00:00, 16.17it/s]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:10<00:21,  3.05s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:20<00:18,  3.64s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:30<00:22,  5.55s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:40<00:10,  5.40s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [00:50<00:06,  6.80s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:00<00:00,  6.04s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:56<00:00, 83.39s/it]\n",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200102075218653\n",
      "there is no More button for comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████▎                                                                          | 1/9 [00:15<02:00, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200102120036824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████▋                                                                 | 2/9 [00:18<01:21, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200102174744517\n",
      "there is no More button for comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|████████████████████████████                                                        | 3/9 [00:33<01:16, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200103091902676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████████▎                                              | 4/9 [00:36<00:48,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200104000505138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████▋                                     | 5/9 [00:39<00:30,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://entertain.v.daum.net/v/20200105154228658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████                            | 6/9 [00:52<00:27,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://entertain.v.daum.net/v/20200105163342087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████▎                  | 7/9 [01:04<00:20, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://entertain.v.daum.net/v/20200105235236310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████▋         | 8/9 [01:17<00:10, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20200105205037906\n",
      "there is no More button for comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:32<00:00, 10.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>press</th>\n",
       "      <th>head</th>\n",
       "      <th>contents</th>\n",
       "      <th>like count</th>\n",
       "      <th>angry count</th>\n",
       "      <th>url</th>\n",
       "      <th>comments count</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020. 01. 02. 07:52</td>\n",
       "      <td>MBC</td>\n",
       "      <td>[스마트 리빙] 미세먼지 심한 날엔 독감 더 잘 걸려요</td>\n",
       "      <td>[뉴스투데이] 미세먼지가 심한 날에는 독감과 같은 호흡기 감염 질환에 걸리지 않도록...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200102075218653</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;p class=\"desc_txt font_size_17\" data-reactid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020. 01. 02. 12:00</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>소비자기본법 40주년..공정위, 아이디어 공모전·학술대회 개최</td>\n",
       "      <td>[세종=뉴시스] 강종민 기자 = 정부세종청사 공정거래위원회. ppkjm@newsis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200102120036824</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020. 01. 02. 17:47</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>中 '폐렴 미스터리' WHO도 조사 나서..숙주는 야생동물?</td>\n",
       "      <td>지난달 31일 원인불명의 바이러스성 폐렴이 발생해 휴업 명령이 내려진 중국 우한시 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200102174744517</td>\n",
       "      <td>11</td>\n",
       "      <td>[&lt;p class=\"desc_txt font_size_17\" data-reactid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020. 01. 03. 09:19</td>\n",
       "      <td>머니S</td>\n",
       "      <td>중국 폐렴 확산.. 제2의 사스 악몽 '공포'</td>\n",
       "      <td>지난달 31일 중국의 한 해산물 시장에서 바이러스성 폐렴이 집단발병해 7명이 중태에...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200103091902676</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020. 01. 04. 00:05</td>\n",
       "      <td>쿠키뉴스</td>\n",
       "      <td>中원인불명 폐렴에 홍콩 비상경보.. 우리는 中정보만 믿고 대응?</td>\n",
       "      <td>사진=CGTN 방송 화면 갈무리\\n최근 중국 후베이 성 우한 시의 화난 수산시장에서...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200104000505138</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020. 01. 05. 15:42</td>\n",
       "      <td>스포츠동아</td>\n",
       "      <td>'미스트롯' 측 \"오늘(5일) 의정부 공연취소, 구조안전상 문제\" [공식입장]</td>\n",
       "      <td>[동아닷컴]\\n‘미스트롯’ 측 “오늘(5일) 의정부 공연취소, 구조안전상 문제”\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200105154228658</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020. 01. 05. 16:33</td>\n",
       "      <td>스포츠조선</td>\n",
       "      <td>[공식] '미스트롯' 전국투어, 5일 의정부 공연 안전상 문제로 취소 \"깊은 사과\"</td>\n",
       "      <td>[스포츠조선 백지은 기자] '미스트롯' 전국투어 콘서트 시즌2 '청춘'(이하 '미스...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200105163342087</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020. 01. 05. 23:52</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>송가인, 미스트롯 의정부 공연 전격 취소에 \"미안하고 감사하다\"</td>\n",
       "      <td>\\n대세 가수 송가인(사진·본명 조은심)이 5일 오후로 예정됐던 ’내일은 미스트롯...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200105235236310</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020. 01. 05. 20:50</td>\n",
       "      <td>동아사이언스</td>\n",
       "      <td>중 후베이성 우한 폐렴 사태..WHO 주시, 전문가들 신종바이러스 우려</td>\n",
       "      <td>중국 후베이성 성도 우한. 크리에이티브커먼스 제공\\n중국 후베이성 성도 우한에서 원...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://v.media.daum.net/v/20200105205037906</td>\n",
       "      <td>6</td>\n",
       "      <td>[&lt;p class=\"desc_txt font_size_17\" data-reactid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date   press  \\\n",
       "0  2020. 01. 02. 07:52     MBC   \n",
       "1  2020. 01. 02. 12:00     뉴시스   \n",
       "2  2020. 01. 02. 17:47     뉴스1   \n",
       "3  2020. 01. 03. 09:19     머니S   \n",
       "4  2020. 01. 04. 00:05    쿠키뉴스   \n",
       "5  2020. 01. 05. 15:42   스포츠동아   \n",
       "6  2020. 01. 05. 16:33   스포츠조선   \n",
       "7  2020. 01. 05. 23:52    세계일보   \n",
       "8  2020. 01. 05. 20:50  동아사이언스   \n",
       "\n",
       "                                             head  \\\n",
       "0                  [스마트 리빙] 미세먼지 심한 날엔 독감 더 잘 걸려요   \n",
       "1              소비자기본법 40주년..공정위, 아이디어 공모전·학술대회 개최   \n",
       "2               中 '폐렴 미스터리' WHO도 조사 나서..숙주는 야생동물?   \n",
       "3                       중국 폐렴 확산.. 제2의 사스 악몽 '공포'   \n",
       "4             中원인불명 폐렴에 홍콩 비상경보.. 우리는 中정보만 믿고 대응?   \n",
       "5     '미스트롯' 측 \"오늘(5일) 의정부 공연취소, 구조안전상 문제\" [공식입장]   \n",
       "6  [공식] '미스트롯' 전국투어, 5일 의정부 공연 안전상 문제로 취소 \"깊은 사과\"   \n",
       "7             송가인, 미스트롯 의정부 공연 전격 취소에 \"미안하고 감사하다\"   \n",
       "8         중 후베이성 우한 폐렴 사태..WHO 주시, 전문가들 신종바이러스 우려   \n",
       "\n",
       "                                            contents like count angry count  \\\n",
       "0  [뉴스투데이] 미세먼지가 심한 날에는 독감과 같은 호흡기 감염 질환에 걸리지 않도록...          0           0   \n",
       "1  [세종=뉴시스] 강종민 기자 = 정부세종청사 공정거래위원회. ppkjm@newsis...          0           0   \n",
       "2  지난달 31일 원인불명의 바이러스성 폐렴이 발생해 휴업 명령이 내려진 중국 우한시 ...          0           0   \n",
       "3  지난달 31일 중국의 한 해산물 시장에서 바이러스성 폐렴이 집단발병해 7명이 중태에...          0           0   \n",
       "4  사진=CGTN 방송 화면 갈무리\\n최근 중국 후베이 성 우한 시의 화난 수산시장에서...          0           0   \n",
       "5  [동아닷컴]\\n‘미스트롯’ 측 “오늘(5일) 의정부 공연취소, 구조안전상 문제”\\n...          0           0   \n",
       "6  [스포츠조선 백지은 기자] '미스트롯' 전국투어 콘서트 시즌2 '청춘'(이하 '미스...          0           0   \n",
       "7   \\n대세 가수 송가인(사진·본명 조은심)이 5일 오후로 예정됐던 ’내일은 미스트롯...          0           0   \n",
       "8  중국 후베이성 성도 우한. 크리에이티브커먼스 제공\\n중국 후베이성 성도 우한에서 원...          0           0   \n",
       "\n",
       "                                           url comments count  \\\n",
       "0  http://v.media.daum.net/v/20200102075218653              1   \n",
       "1  http://v.media.daum.net/v/20200102120036824              0   \n",
       "2  http://v.media.daum.net/v/20200102174744517             11   \n",
       "3  http://v.media.daum.net/v/20200103091902676              0   \n",
       "4  http://v.media.daum.net/v/20200104000505138              0   \n",
       "5  http://v.media.daum.net/v/20200105154228658              0   \n",
       "6  http://v.media.daum.net/v/20200105163342087              0   \n",
       "7  http://v.media.daum.net/v/20200105235236310              0   \n",
       "8  http://v.media.daum.net/v/20200105205037906              6   \n",
       "\n",
       "                                            comments  \n",
       "0  [<p class=\"desc_txt font_size_17\" data-reactid...  \n",
       "1                                                     \n",
       "2  [<p class=\"desc_txt font_size_17\" data-reactid...  \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                                     \n",
       "7                                                     \n",
       "8  [<p class=\"desc_txt font_size_17\" data-reactid...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_list = []\n",
    "dates_list= []\n",
    "heads_list = []\n",
    "press_list = []\n",
    "contents_list = []\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "for k in tqdm(url):\n",
    "    driver.get(k)\n",
    "    \n",
    "    urls_list = sum( [urls_list, get_article_url()], [])\n",
    "    \n",
    "for article_url in urls_list:\n",
    "    \n",
    "    driver.get(article_url)    \n",
    "    contents_list.append(get_article_contents())\n",
    "    time.sleep(1)\n",
    "    heads_list.append(get_article_head())\n",
    "    time.sleep(1)\n",
    "    dates_list.append(get_article_date())\n",
    "    time.sleep(1)\n",
    "    press_list.append(get_press_name())\n",
    "    time.sleep(1)\n",
    "    tmp1, tmp2 = get_article_centiments()\n",
    "    centiments_like_list.append(tmp1)\n",
    "    centiments_angry_list.append(tmp2)\n",
    "    time.sleep(3)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([dates_list, press_list, heads_list, contents_list, centiments_like_list, centiments_angry_list, urls_list]).T\n",
    "df.columns = ['date', 'press', 'head', 'contents', 'like count', 'angry count', 'url']\n",
    "\n",
    "df['like count'] = df['like count'].replace('-', '0')\n",
    "df['angry count'] = df['angry count'].replace('-', '0')\n",
    "\n",
    "comments_list = []\n",
    "comments_num = []\n",
    "\n",
    "for article_url in tqdm(urls_list):\n",
    "    driver.get(article_url)\n",
    "    print(driver.current_url)\n",
    "\n",
    "    try:\n",
    "        num_imsi = driver.find_element_by_xpath('//*[@id=\"alex-header\"]/em').text\n",
    "        comments_num.append(num_imsi) \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        comments_num.append('0')\n",
    "    \n",
    "    if num_imsi == '0':\n",
    "        comments_list.append('no comments')\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').text != '':\n",
    "            while cnt < 2:\n",
    "                driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').click()\n",
    "                cnt += 1\n",
    "            time.sleep(5)\n",
    "    except:\n",
    "        print('there is no More button for comments')\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        comments = dom.find_all(\"p\", \"desc_txt font_size_17\", limit= 10) \n",
    "        comments_list.append(comments)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('comments text crawling error')\n",
    "        comments_list.append(' ')\n",
    "        pass\n",
    "    \n",
    "df['comments count'] = comments_num\n",
    "df['comments'] = comments_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file_name = '다음뉴스_코로나_2021년_7월_크롤링_{}.xlsx'.format(date)\n",
    "df.to_excel(xlsx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_url_list():\n",
    "    for i in range(20210801, 20210832):\n",
    "        etl_ymd = str(i)\n",
    "        url.append('https://search.daum.net/search?nil_suggest=btn&w=news&DA=PGD&q=%EC%BD%94%EB%A1%9C%EB%82%98&p=1&period=u&sd='+etl_ymd+'000000&ed='+etl_ymd+'235959')\n",
    "    return url\n",
    "\n",
    "url = []\n",
    "get_url_list()\n",
    "\n",
    "def get_article_url():\n",
    "    \n",
    "    urls_list = []\n",
    "    \n",
    "    for i in tqdm(range(1, 11)):\n",
    "        try:\n",
    "            order_num = str(i)\n",
    "            article_url = driver.find_elements_by_xpath('//*[@id=\"newsColl\"]/div[1]/ul/li['+order_num+']/div[2]/span[1]/a')\n",
    "            urls_list.append(article_url[0].get_attribute('href'))    \n",
    "            time.sleep(3)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return urls_list\n",
    "\n",
    "heads_list = []\n",
    "\n",
    "def get_article_head():\n",
    "    \n",
    "    try:\n",
    "        article_head = driver.find_elements_by_css_selector('#cSub > div > h3')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_head = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_head\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "def get_article_date():\n",
    "    \n",
    "    try:\n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        article_date = dom.find(\"span\", \"num_date\").text \n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_date = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_date\n",
    "\n",
    "press_list = []\n",
    "\n",
    "def get_press_name():\n",
    "\n",
    "    try:\n",
    "        press_name = driver.find_elements_by_css_selector('#cSub > div > em > a > img')[0].get_attribute(\"alt\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        press_name = ' '\n",
    "        pass\n",
    "    \n",
    "    return press_name\n",
    "\n",
    "contents_list = []\n",
    "\n",
    "def get_article_contents():\n",
    "    \n",
    "    try:\n",
    "        article_content = driver.find_elements_by_xpath('//*[@id=\"harmonyContainer\"]')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_content = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_content\n",
    "\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "def get_article_centiments():\n",
    "        \n",
    "    try:\n",
    "        article_centiment_like = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-LIKE.unselected > span.count')[0].text\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        article_centiment_like = ' '\n",
    "        pass\n",
    "    \n",
    "    time.sleep(3)\n",
    "        \n",
    "    try:\n",
    "        article_centiment_angry = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-ANGRY.unselected > span.count')[0].text\n",
    "        time.sleep(3)        \n",
    "    except:\n",
    "        article_centiment_angry = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_centiment_like, article_centiment_angry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_list = []\n",
    "dates_list= []\n",
    "heads_list = []\n",
    "press_list = []\n",
    "contents_list = []\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "for k in tqdm(url):\n",
    "    driver.get(k)\n",
    "    \n",
    "    urls_list = sum( [urls_list, get_article_url()], [])\n",
    "    \n",
    "for article_url in urls_list:\n",
    "    \n",
    "    driver.get(article_url)    \n",
    "    contents_list.append(get_article_contents())\n",
    "    time.sleep(1)\n",
    "    heads_list.append(get_article_head())\n",
    "    time.sleep(1)\n",
    "    dates_list.append(get_article_date())\n",
    "    time.sleep(1)\n",
    "    press_list.append(get_press_name())\n",
    "    time.sleep(1)\n",
    "    tmp1, tmp2 = get_article_centiments()\n",
    "    centiments_like_list.append(tmp1)\n",
    "    centiments_angry_list.append(tmp2)\n",
    "    time.sleep(3)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([dates_list, press_list, heads_list, contents_list, centiments_like_list, centiments_angry_list, urls_list]).T\n",
    "df.columns = ['date', 'press', 'head', 'contents', 'like count', 'angry count', 'url']\n",
    "\n",
    "df['like count'] = df['like count'].replace('-', '0')\n",
    "df['angry count'] = df['angry count'].replace('-', '0')\n",
    "\n",
    "comments_list = []\n",
    "comments_num = []\n",
    "\n",
    "for article_url in tqdm(urls_list):\n",
    "    driver.get(article_url)\n",
    "    print(driver.current_url)\n",
    "\n",
    "    try:\n",
    "        num_imsi = driver.find_element_by_xpath('//*[@id=\"alex-header\"]/em').text\n",
    "        comments_num.append(num_imsi) \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        comments_num.append('0')\n",
    "    \n",
    "    if num_imsi == '0':\n",
    "        comments_list.append('no comments')\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').text != '':\n",
    "            while cnt < 2:\n",
    "                driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').click()\n",
    "                cnt += 1\n",
    "            time.sleep(5)\n",
    "    except:\n",
    "        print('there is no More button for comments')\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        comments = dom.find_all(\"p\", \"desc_txt font_size_17\", limit= 10) \n",
    "        comments_list.append(comments)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('comments text crawling error')\n",
    "        comments_list.append(' ')\n",
    "        pass\n",
    "    \n",
    "df['comments count'] = comments_num\n",
    "df['comments'] = comments_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file_name = '다음뉴스_코로나_2021년_8월_크롤링_{}.xlsx'.format(date)\n",
    "df.to_excel(xlsx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_url_list():\n",
    "    for i in range(20210901, 20210931):\n",
    "        etl_ymd = str(i)\n",
    "        url.append('https://search.daum.net/search?nil_suggest=btn&w=news&DA=PGD&q=%EC%BD%94%EB%A1%9C%EB%82%98&p=1&period=u&sd='+etl_ymd+'000000&ed='+etl_ymd+'235959')\n",
    "    return url\n",
    "\n",
    "url = []\n",
    "get_url_list()\n",
    "\n",
    "def get_article_url():\n",
    "    \n",
    "    urls_list = []\n",
    "    \n",
    "    for i in tqdm(range(1, 11)):\n",
    "        try:\n",
    "            order_num = str(i)\n",
    "            article_url = driver.find_elements_by_xpath('//*[@id=\"newsColl\"]/div[1]/ul/li['+order_num+']/div[2]/span[1]/a')\n",
    "            urls_list.append(article_url[0].get_attribute('href'))    \n",
    "            time.sleep(3)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return urls_list\n",
    "\n",
    "heads_list = []\n",
    "\n",
    "def get_article_head():\n",
    "    \n",
    "    try:\n",
    "        article_head = driver.find_elements_by_css_selector('#cSub > div > h3')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_head = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_head\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "def get_article_date():\n",
    "    \n",
    "    try:\n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        article_date = dom.find(\"span\", \"num_date\").text \n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_date = ' '\n",
    "        pass\n",
    "    \n",
    "    return article_date\n",
    "\n",
    "press_list = []\n",
    "\n",
    "def get_press_name():\n",
    "\n",
    "    try:\n",
    "        press_name = driver.find_elements_by_css_selector('#cSub > div > em > a > img')[0].get_attribute(\"alt\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        press_name = ' '\n",
    "        pass\n",
    "    \n",
    "    return press_name\n",
    "\n",
    "contents_list = []\n",
    "\n",
    "def get_article_contents():\n",
    "    \n",
    "    try:\n",
    "        article_content = driver.find_elements_by_xpath('//*[@id=\"harmonyContainer\"]')[0].text\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        article_content = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_content\n",
    "\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "def get_article_centiments():\n",
    "        \n",
    "    try:\n",
    "        article_centiment_like = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-LIKE.unselected > span.count')[0].text\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        article_centiment_like = ' '\n",
    "        pass\n",
    "    \n",
    "    time.sleep(3)\n",
    "        \n",
    "    try:\n",
    "        article_centiment_angry = driver.find_elements_by_css_selector(\n",
    "            '#mArticle > div.foot_view > div.emotion_wrap > div.emotion_list > div > div > div > div.selectionbox.type-ANGRY.unselected > span.count')[0].text\n",
    "        time.sleep(3)        \n",
    "    except:\n",
    "        article_centiment_angry = ' '\n",
    "        pass\n",
    "        \n",
    "    return article_centiment_like, article_centiment_angry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_list = []\n",
    "dates_list= []\n",
    "heads_list = []\n",
    "press_list = []\n",
    "contents_list = []\n",
    "centiments_like_list, centiments_angry_list = [], []\n",
    "\n",
    "for k in tqdm(url):\n",
    "    driver.get(k)\n",
    "    \n",
    "    urls_list = sum( [urls_list, get_article_url()], [])\n",
    "    \n",
    "for article_url in urls_list:\n",
    "    \n",
    "    driver.get(article_url)    \n",
    "    contents_list.append(get_article_contents())\n",
    "    time.sleep(1)\n",
    "    heads_list.append(get_article_head())\n",
    "    time.sleep(1)\n",
    "    dates_list.append(get_article_date())\n",
    "    time.sleep(1)\n",
    "    press_list.append(get_press_name())\n",
    "    time.sleep(1)\n",
    "    tmp1, tmp2 = get_article_centiments()\n",
    "    centiments_like_list.append(tmp1)\n",
    "    centiments_angry_list.append(tmp2)\n",
    "    time.sleep(3)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([dates_list, press_list, heads_list, contents_list, centiments_like_list, centiments_angry_list, urls_list]).T\n",
    "df.columns = ['date', 'press', 'head', 'contents', 'like count', 'angry count', 'url']\n",
    "\n",
    "df['like count'] = df['like count'].replace('-', '0')\n",
    "df['angry count'] = df['angry count'].replace('-', '0')\n",
    "\n",
    "comments_list = []\n",
    "comments_num = []\n",
    "\n",
    "for article_url in tqdm(urls_list):\n",
    "    driver.get(article_url)\n",
    "    print(driver.current_url)\n",
    "\n",
    "    try:\n",
    "        num_imsi = driver.find_element_by_xpath('//*[@id=\"alex-header\"]/em').text\n",
    "        comments_num.append(num_imsi) \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        comments_num.append('0')\n",
    "    \n",
    "    if num_imsi == '0':\n",
    "        comments_list.append('no comments')\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').text != '':\n",
    "            while cnt < 2:\n",
    "                driver.find_element_by_css_selector('#alex-area > div > div > div > div.cmt_box > div.alex_more > button').click()\n",
    "                cnt += 1\n",
    "            time.sleep(5)\n",
    "    except:\n",
    "        print('there is no More button for comments')\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        html = driver.page_source\n",
    "        dom = bs(html, 'lxml')\n",
    "        comments = dom.find_all(\"p\", \"desc_txt font_size_17\", limit= 10) \n",
    "        comments_list.append(comments)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('comments text crawling error')\n",
    "        comments_list.append(' ')\n",
    "        pass\n",
    "    \n",
    "df['comments count'] = comments_num\n",
    "df['comments'] = comments_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file_name = '다음뉴스_코로나_2021년_9월_크롤링_{}.xlsx'.format(date)\n",
    "df.to_excel(xlsx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
